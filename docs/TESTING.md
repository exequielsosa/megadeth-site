# ğŸ§ª GUÃA DE PRUEBAS - SISTEMA DE SCRAPING AUTOMATIZADO

## ğŸ“‹ PREPARACIÃ“N

### 1. Verificar Variables de Entorno

Primero, verifica que todas las variables necesarias estÃ©n configuradas:

```bash
npm run check:config
```

**Variables requeridas:**
- `NEXT_PUBLIC_SUPABASE_URL`
- `NEXT_PUBLIC_SUPABASE_ANON_KEY`
- `SUPABASE_SERVICE_KEY`
- `NEWS_API_KEY`
- `NEWS_API_URL` (debe ser `http://localhost:3000/api/news/create` para pruebas locales)
- `GEMINI_API_KEY`

Si falta alguna, copia el archivo de ejemplo:
```bash
cp .env.example .env
```

Y edita `.env` con tus valores reales.

---

## ğŸš€ PRUEBA LOCAL

### Paso 1: Iniciar el Servidor de Desarrollo

En una terminal PowerShell:

```powershell
npm run dev
```

Espera a que aparezca:
```
âœ“ Ready in 2.3s
â—‹ Local:        http://localhost:3000
```

**âš ï¸ IMPORTANTE:** Deja esta terminal abierta durante toda la prueba.

---

### Paso 2: Ejecutar el Scraper

En una **SEGUNDA terminal PowerShell** (nueva ventana):

```powershell
npm run scrape:news
```

---

## ğŸ“Š QUÃ‰ VAS A VER

### Salida Esperada del Scraper:

```
ğŸ¤– Scraper de Noticias de Megadeth - Inicio
================================================
ğŸ¯ Feeds a procesar: 4

ğŸ“¡ Procesando feed: https://www.blabbermouth.net/feed/
   Encontrados 20 items
   ğŸ” Analizando: "Dave Mustaine talks about new album..."
   âœ… Relevante para Megadeth
   ğŸ” Analizando: "Metallica announces tour dates..."
   â­ï¸  No es relevante
   
ğŸ¤– Procesando con Gemini AI: "Dave Mustaine talks about new album..."
   âœ… Procesamiento AI completado

ğŸ“¤ Creando noticia: "Dave Mustaine habla sobre el nuevo Ã¡lbum..."
   âœ… Noticia creada exitosamente: dave-mustaine-talks-about-new-alb-1739433600000

================================================
ğŸ“Š RESUMEN FINAL
================================================
âœ… Noticias encontradas relevantes: 5
âœ… Noticias creadas exitosamente: 3
â­ï¸  Noticias duplicadas (omitidas): 2
âŒ Errores: 0
```

---

## âœ… VERIFICACIÃ“N DE RESULTADOS

### 1. Verificar en el Sitio Web

1. Ve a http://localhost:3000/noticias
2. DeberÃ­as ver las nuevas noticias creadas
3. Los tÃ­tulos deben estar en espaÃ±ol e inglÃ©s
4. Las descripciones deben ser completas (200-400 palabras)

### 2. Verificar en Supabase

1. Ve a https://supabase.com/dashboard
2. Abre tu proyecto â†’ Table Editor â†’ `news_articles`
3. Verifica que aparecen las nuevas filas
4. Campos importantes:
   - `is_automated = true`
   - `source_url` debe tener la URL original del RSS
   - `image_url` debe tener imagen o fallback `/images/band.webp`
   - `youtube_video_id` si la noticia tenÃ­a video embebido

---

## ğŸ”§ TROUBLESHOOTING

### Error: "API Key invÃ¡lida"
```
âŒ Error 401: API Key invÃ¡lida o no proporcionada
```

**SoluciÃ³n:** Verifica que `NEWS_API_KEY` en `.env` coincida con el valor configurado.

---

### Error: "Gemini API Key not configured"
```
âŒ Error: Gemini API Key not configured
```

**SoluciÃ³n:** 
1. Ve a https://makersuite.google.com/app/apikey
2. Crea una API Key
3. AgrÃ©gala a `.env` como `GEMINI_API_KEY=tu-key-aqui`

---

### Error: "Connection refused"
```
âŒ Error: fetch failed - ECONNREFUSED
```

**SoluciÃ³n:** El servidor de desarrollo no estÃ¡ corriendo. Ejecuta `npm run dev` en otra terminal.

---

### No encuentra noticias relevantes
```
ğŸ“Š RESUMEN: Noticias encontradas relevantes: 0
```

**Causas posibles:**
1. Los feeds RSS no tienen noticias recientes sobre Megadeth
2. Las keywords son demasiado estrictas (poco probable con 55+ keywords)
3. Gemini AI estÃ¡ siendo muy conservador

**SoluciÃ³n:** Ejecuta de nuevo en unos dÃ­as o cuando haya noticias frescas de Megadeth.

---

### Todas las noticias son duplicadas
```
â­ï¸  Noticias duplicadas (omitidas): 5
```

**Normal:** Las noticias ya fueron scrapeadas antes (por `source_url`).

**Para probar de nuevo:** 
1. Borra las noticias de prueba en Supabase
2. Ejecuta de nuevo `npm run scrape:news`

---

## ğŸ“ˆ MÃ‰TRICAS DE RENDIMIENTO

**Tiempo estimado:**
- 4 feeds RSS: ~5-10 segundos para descargar
- Filtrado keywords: instantÃ¡neo
- Gemini AI relevancia (5-10 noticias): ~10-20 segundos
- Gemini AI procesamiento (3-5 relevantes): ~15-30 segundos
- CreaciÃ³n API (3-5 noticias): ~2-5 segundos

**Total: 30-60 segundos por ejecuciÃ³n**

---

## ğŸ¯ PRÃ“XIMO PASO: ACTIVAR AUTOMATIZACIÃ“N

Una vez que todo funciona localmente, configura GitHub Actions:

```bash
# 1. Push a GitHub
git add .
git commit -m "feat: add automated news scraping system"
git push origin develop

# 2. Configurar GitHub Secrets
Repository â†’ Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret

Agregar:
- NEWS_API_URL: https://tu-dominio.com/api/news/create
- NEWS_API_KEY
- GEMINI_API_KEY
- NEXT_PUBLIC_SUPABASE_URL
- NEXT_PUBLIC_SUPABASE_ANON_KEY
- SUPABASE_SERVICE_KEY

# 3. Activar workflow
Actions â†’ Scrape Megadeth News â†’ Run workflow
```

---

## ğŸ“… CALENDARIO AUTOMÃTICO

Una vez configurado GitHub Actions:
- ğŸ—“ï¸ **Martes 10:00 AM UTC**: EjecuciÃ³n automÃ¡tica
- ğŸ—“ï¸ **Viernes 10:00 AM UTC**: EjecuciÃ³n automÃ¡tica
- âš¡ **Manual**: Cualquier momento desde GitHub Actions

---

Â¿Listo para probar? Ejecuta:
```bash
npm run check:config
```
